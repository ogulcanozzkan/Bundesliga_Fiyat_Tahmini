# -*- coding: utf-8 -*-
"""bundesliga_player

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mXBanzQJ4w3aZGiLs76EedjES-EvBTKW

# 1. Introduction
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use("seaborn-whitegrid")
import seaborn as sns
from collections import Counter
import warnings
warnings.filterwarnings("ignore")
from datetime import datetime, date, time

"""# 2. Load and Check Data"""

data = pd.read_csv("/content/bundesliga_player.csv")

data.head()

data["position"].value_counts()

data.info()

def bar_plot(variable):
    """
        input: variable ex: "Sex"
        output: bar plot & value count
    """
    # get feature
    var = data[variable]
    # count number of categorical variable(value/sample)
    varValue = var.value_counts()

    # visualize
    plt.figure(figsize = (9,3))
    plt.bar(varValue.index, varValue)
    plt.xticks(varValue.index, varValue.index.values)
    plt.ylabel("Frequency")
    plt.title(variable)
    plt.show()
    print("{}: \n {}".format(variable,varValue))

category1 = ["price","max_price"]
for c in category1:
    bar_plot(c)

"""# 3. Basic Data Analysis"""

data.head()

data[["price","max_price"]].groupby(["price"], as_index = False).mean().sort_values(by="price",ascending = False)

data[["price","foot"]].groupby(["foot"], as_index = False).mean().sort_values(by="price",ascending = False)

data[["price","shirt_nr"]].groupby(["shirt_nr"], as_index = False).mean().sort_values(by="price",ascending = False)

data[["price","club"]].groupby(["club"], as_index = False).mean().sort_values(by="price",ascending = False)

"""# 4.Outlier Detection"""

def detect_outliers(df,features):
    outlier_indices = []

    for c in features:
        # 1st quartile
        Q1 = np.percentile(df[c],5)
        # 3rd quartile
        Q3 = np.percentile(df[c],95)
        # IQR
        IQR = Q3 - Q1
        # Outlier step
        outlier_step = IQR * 1.5
        # detect outlier and their indeces
        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index
        # store indeces
        outlier_indices.extend(outlier_list_col)

    outlier_indices = Counter(outlier_indices)
    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)

    return multiple_outliers

data.loc[detect_outliers(data,["max_price","age","price"])]

"""# 5. Missing Value"""

data.isnull().sum()

data = data.drop("full_name",axis="columns")
data = data.drop("Unnamed: 0",axis="columns")

data.head()

data.isnull().sum()
data[data["contract_expires"].isnull()]

data.boxplot(column="max_price",by = "contract_expires")
plt.show()

data["contract_expires"] = data["contract_expires"].fillna("2024-06-30")
data[data["contract_expires"].isnull()]

data.isnull().sum()

data[data["place_of_birth"].isnull()]

data = data.dropna(subset=['place_of_birth'])

data[data["place_of_birth"].isnull()]

data.isnull().sum()

data[data["price"].isnull()]

data = data.dropna(subset=['price'])

data.isnull().sum()

data[data["foot"].isnull()]

data["foot"] = data["foot"].fillna("right")

data.isnull().sum()

data[data["player_agent"].isnull()]

data["player_agent"] = data["player_agent"].fillna("other")

data["outfitter"] = data["outfitter"].fillna("None")

"""# 6.Visualization"""

sns.countplot(x=data['outfitter'],color='green')
plt.title('Brand Sponsors')
plt.xticks(rotation=90)

data.head()

data = data.drop("place_of_birth",axis=1)

sns.countplot(x=data['max_price'])
plt.title('Max Price')
plt.show()

sns.countplot(x=data['age'])
plt.title('Distribution of ages in Bundesliga')
plt.show()

sns.heatmap(data.corr(), annot = True, fmt = ".2f")
plt.show()

plt.figure(figsize=(50, 20))
sns.catplot(x="price", y="max_price", data=data, kind="bar")
plt.show()

plt.figure(figsize=(50, 20))
sns.catplot(x="price", y="NEW_AGE", data=data, kind="bar")
plt.show()

plt.figure(figsize=(50, 20))
sns.catplot(x="price", y="NEW_POSITION", data=data, kind="bar")
plt.show()

plt.figure(figsize=(50, 20))
sns.catplot(x="max_price", y="NEW_POSITION", data=data, kind="bar")
plt.show()

plt.figure(figsize=(10, 50))
sns.catplot(x="max_price", y="age", data=data,kind="bar")
plt.show()

list1 = ["price","max_price","age","NEW_POSITION"]
sns.heatmap(data[list1].corr(), annot = True, fmt = ".2f")
plt.show()

x = data
sns.lineplot(x="age", y="max_price", data=data)
plt.show()

x = data
sns.lineplot(x="age", y="price", data=data)
plt.show()

plt.figure(figsize=(12, 7))
sns.scatterplot(x="age", y="max_price", hue=data['NEW_POSITION'], data=data)
plt.show()

"""# 7. Feature Engineering"""

data["position"].value_counts()

data.loc[(data["position"] == "Goalkeeper"), "NEW_POSITION" ]= "Goalkeeper"


data.loc[(data["position"] == "Defender - Centre-Back"), "NEW_POSITION" ]= "defender"
data.loc[(data["position"] == "Defender - Right-Back"), "NEW_POSITION" ]= "defender"
data.loc[(data["position"] == "Defender - Left-Back"), "NEW_POSITION" ]= "defender"

data.loc[(data["position"] == "midfield - Central Midfield "), "NEW_POSITION" ]= "Midfield"
data.loc[(data["position"] == "midfield - Defensive Midfield"), "NEW_POSITION" ]= "Midfield"
data.loc[(data["position"] == "midfield - Attacking Midfield"), "NEW_POSITION" ]= "Midfield"
data.loc[(data["position"] == "midfield - Right Midfield"), "NEW_POSITION" ]= "Midfield"
data.loc[(data["position"] == "midfield - Left Midfield "), "NEW_POSITION" ]= "Midfield"

data.loc[(data["position"] == "Attack - Centre-Forward "), "NEW_POSITION" ]= "Attack"
data.loc[(data["position"] == "Attack - Right Winger"), "NEW_POSITION" ]= "Attack"
data.loc[(data["position"] == "Attack - Second Striker"), "NEW_POSITION" ]= "Attack"
data.loc[(data["position"] == "Attack - Second Striker"), "NEW_POSITION" ]= "Attack"

data.loc[(data["outfitter"] == "Uhlsport"), "outfitter" ]= "other"
data.loc[(data["outfitter"] == "Under Armour"), "outfitter" ]= "other"
data.loc[(data["outfitter"] == "New Balance"), "outfitter" ]= "other"
data.loc[(data["outfitter"] == "HashtagOne"), "outfitter" ]= "other"
data.loc[(data["outfitter"] == "Stanno"), "outfitter" ]= "other"
data.loc[(data["outfitter"] == "Mizuno"), "outfitter" ]= "other"

data = data.drop("position",axis=1)

data.head()

pd.DataFrame(data["nationality"])

data["NEW_AGE"] = pd.cut(data["age"],bins=[17,23,30,35,44],labels=["young","adult","senior","old"])

data["contract_expires"] = pd.to_datetime(data["contract_expires"])
data["joined_club"] = pd.to_datetime(data["joined_club"])
data["CONTRACT_DURATION"] = data["contract_expires"] - data["joined_club"]
data["CONTRACT_DURATION"] = data["CONTRACT_DURATION"].dt.days / 365

data["club"].value_counts()

data.info()

data["age"].value_counts()

sns.heatmap(data.corr(), annot = True, fmt = ".2f")
plt.show()

data.describe()

data.head()

data["player_agent"].value_counts()

data["nationality"].value_counts()

data = data.drop("player_agent",axis=1)
data.head()

data = data.drop("nationality",axis=1)
data.head()

data = pd.get_dummies(data,columns=["club"])
data.head()

data = pd.get_dummies(data,columns=["outfitter","NEW_POSITION","NEW_AGE"])
data.head()

data = pd.get_dummies(data,columns=["foot"])
data.head()

data = data.drop(["name","contract_expires","joined_club"],axis=1)

data.head()

"""# 8. Modeling"""

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import GradientBoostingRegressor

data.info()

data = data.drop("place_of_birth",axis=1)
data.head()

data_len = len(data)
print(data_len)

X = data.drop(["price"],axis=1)
X.head()

y = data[["price"]]
y.head()

"""# Random Forest

"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)
rf_model = RandomForestRegressor(random_state=42).fit(X_train,y_train)

rf_model

y_pred = rf_model.predict(X_test)
np.sqrt(mean_squared_error(y_test,y_pred))

rf_params = {"max_depth": [8,10],
            "max_features": [2,5,10],
             "n_estimators": [200,500,1000],
             "min_samples_split": [2,10,80]}

rf_cv_model = GridSearchCV(rf_model,rf_params,cv = 10 ,n_jobs=-1,verbose=2).fit(X_train,y_train)

rf_cv_model.best_params_

rf_model = RandomForestRegressor(random_state=42,max_depth = 10,max_features= 10,min_samples_split= 2,n_estimators= 200).fit(X_train,y_train)
rf_tuned = rf_model.fit(X_train,y_train)

rf_tuned

y_pred = rf_tuned.predict(X_test)
np.sqrt(mean_squared_error(y_test,y_pred))

Importance = pd.DataFrame({"Importance":rf_tuned.feature_importances_*100},
                          index = X_train.columns)

Importance.sort_values(by="Importance",
                       axis=0,
                       ascending=True).plot(kind="barh",color="r")

plt.xlabel("Variable Ä°mportance")
plt.gca().legend_ = None

"""# GBM"""

gbm_model = GradientBoostingRegressor().fit(X_train,y_train)
gbm_model

y_pred = gbm_model.predict(X_test)
np.sqrt(mean_squared_error(y_test,y_pred))

?gbm_model

gbm_params = {"learning_rate": [0.001,0.1,0.01],
              "max_depth": [3,8],
              "n_estimators": [200,500],
              "subsample": [1,0.5,0.8],
              "loss": ["ls","lad","quantile"]}

gbm_model = GradientBoostingRegressor().fit(X_train,y_train)

gbm_cv_model = GridSearchCV(gbm_model,gbm_params,cv=10,n_jobs=-1,verbose=2).fit(X_train,y_train)

gbm_cv_model.best_params_

gbm_tuned = GradientBoostingRegressor(learning_rate = 0.1,
                                      loss = "quantile",
                                      max_depth = 8,
                                      n_estimators = 500,
                                      subsample = 0.5).fit(X_train,y_train)

y_pred = gbm_tuned.predict(X_test)
np.sqrt(mean_squared_error(y_test,y_pred))

Importance = pd.DataFrame({"Importance":gbm_tuned.feature_importances_*100},
                          index = X_train.columns)

Importance.sort_values(by="Importance",
                       axis=0,
                       ascending=True).plot(kind="barh",color="r")

plt.xlabel("Variable Ä°mportance")
plt.gca().legend_ = None

"""# XGBoost"""

import xgboost
from xgboost import XGBRegressor

xgb = XGBRegressor().fit(X_train,y_train)
xgb

y_pred = xgb.predict(X_test)
np.sqrt(mean_squared_error(y_test,y_pred))

xgb_params = {"learning_rate":[0.1,0.01,0.5],
              "max_depth": [2,3,5,8],
              "n_estimators": [100,200,500,1000],
              "colsample_bytree": [0.4,0.7,1]}

xgb_cv_model = GridSearchCV(xgb,xgb_params,cv = 10,n_jobs=-1,verbose=2).fit(X_train,y_train)